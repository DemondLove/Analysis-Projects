---
title: "DSC 520 Assignment 6.1 - Housing Data"
author: "Demond Love"
date: "7/13/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Data for this assignment is focused on real estate transactions recorded from 1964 to 2016 and can be found in Week 6 Housing.xlsx. Using your skills in statistical correlation, multiple regression and R programming, you are interested in the following variables: Sale Price and several other possible predictors.**
```{r, results='hide', warning = FALSE, message = FALSE}
library(readxl)
library(ggplot2)
library(pastecs)
library(ggm)
library(lm.beta)
library(lmtest)
library(QuantPsyc)
library(car)
file = read_excel("/Users/Love/Documents/DSC 520 Statistics for Data Science/Week 6 DSC 520/week-6-housing.xlsx") 
```

**a.	Explain why you chose to remove data points from your ‘clean’ dataset.**
```{r}
data = data.frame(file$`Sale Price`, file$sq_ft_lot)
names(data) = c("sale_price", "sq_footage")
simpledataset = data[!(data$sale_price <= 14000 & data$sq_footage > 50000),]
str(simpledataset)
```

Last week I identified outliers in which the price of the home was less than $14,001, but the home had more than 50,000 sqare feet. These cases were removed from the dataset, since there must be a data entry error (+50,000 square foot properties will almost never sell for this small amount), there are a few of them so don't they drastically impact the underlying integrity of the sample, and I beleive these not to be from the population
that we intended for this sample.

**b.	Create two variables; one that will contain the variables Sale Price and Square Foot of Lot (same variables used from previous assignment on simple regression) and one that will contain Sale Price and several additional predictors of your choice. Explain the basis for your additional predictor selections.**
```{r}
multipledataset = data.frame(file$`Sale Price`, file$sq_ft_lot, file$bedrooms, file$bath_full_count)
names(multipledataset) = c("sale_price", "sq_footage", "bedroom", "fullbath")
multipledataset = multipledataset[!(data$sale_price <= 14000 & data$sq_footage > 50000),]
str(multipledataset)
```

I have chosen to include number of bedrooms, number of full bathrooms, and number of half bathrooms to my dataset, instead of square footage. I have chosen these variables because they are the variables that househunters most look for in a home.

**c.	Execute a summary() function on two variables defined in the previous step to compare the model results. What are the R2 and Adjusted R2 statistics? Explain what these results tell you about the overall model. Did the inclusion of the additional predictors help explain any large variations found in Sale Price?**

The value of R-squared is a measure of how much of the variability in the outcome is accounted for by the predictors. 

The adjusted R-squared gives us some idea of how well our model generalizes, and ideally we would like its value to be the same, or very close to, the value of R-squared.
```{r}
housingsimplemodel = lm(sale_price ~ sq_footage, simpledataset)
summary(housingsimplemodel)
```

```{r}
housingmultimodel = lm(sale_price ~ sq_footage + bedroom + fullbath, multipledataset)
summary(housingmultimodel)
```

Whereas the R-squared for the simple linear regression model is 0.02 and the adjusted R-squared is also 0.02. Therefore, only 2% of the variability is explained with the by the single predictor of this model.


On the other hand, the R-squared for the multiple regression model is 0.12 and the adjusted R-squared is also 0.12. Therefore, about 6x of the variability is explained with by the predictors of this model.

**d.	Considering the parameters of the multiple regression model you have created. What are the standardized betas for each parameter and what do the values indicate?**
```{r}
lm.beta(housingmultimodel)
```
These are the standardized regression coefficients, which are not dependent on the units of measurement of the variables. These estimates tell us the number of standard deviations by which the outcome will change as a result of one standard devisation change in the predictor. The standardized beta values are all measured in standard deviation units and so are directly comparable: therefore, they provide a better insight into the 'importance' of a predictor in the model.


Here, you can clearly see a deligniation of variables that are chosen. The number of full bathrooms are by far the most important, followed by the number of bedrooms, and the square footage falling well behind. This is a little surprising, since I would have thought the number of bedrooms would have lead the path in order of importance.


**e.	Calculate the confidence intervals for the parameters in your model and explain what the results indicate.**
```{r}
confidenceintervals = confint(housingmultimodel)
confidenceintervals
```
The number one thing we are looking for here is if the confidence interval crosses 0. If one is positive and another is negative, then that means that we have a bad model. However, all four predictors have a positive relationship, which doesn't cross zero, which means that these are all statistically significant.


**f.	Assess the improvement of the new model compared to your original model (simple regression model) by testing whether this change is significant by performing an analysis of variance.**
```{r}
comparingmodels = anova(housingsimplemodel, housingmultimodel)
comparingmodels
```
Here, you can see that the RSS for the second model is lower than the first, meaning it is a better model. However, the problem with RSS is that it always improves when an additional variable is added to the model.

**g.	Perform casewise diagnostics to identify outliers and/or influential cases, storing each function's output in a dataframe assigned to a unique variable name. h.	Calculate the standardized residuals using the appropriate command, specifying those that are +-2, storing the results of large residuals in a variable you create.**
```{r}
multipledataset$standardized.residuals = rstandard(housingmultimodel)
multipledataset$large.residuals = multipledataset$standardized.residuals > 2 | multipledataset$standardized.residuals < -2
```

**i.	Use the appropriate function to show the sum of large residuals.**
```{r}
sum(multipledataset$large.residuals)
```

**j.	Which specific variables have large residuals (only cases that evaluate as TRUE)?**
```{r}
LARGES = subset(multipledataset, large.residuals == TRUE)
```

**k.	Investigate further by calculating the leverage, cooks distance, and covariance rations. Comment on all cases that are problematics.**

Here we are looking for Cook's distances above 1, leverage values that are twice the average leverage, and cases falling outside of the upper and lower CVR limits (defined as 1 plus three times the average leverage, whereas the lower limit is 1 minus three times the average leverage).

```{r}
multipledataset$cooks.distance = cooks.distance(housingmultimodel)
cooksdistanceoutliers = which(multipledataset$cooks.distance > 1)
multipledataset[cooksdistanceoutliers,]
```
This is a property with 23 full bathrooms and only 4 bedrooms. This clearly is not a residential property, and therefore is not a part of our desired population.

```{r}
multipledataset$leverage = hatvalues(housingmultimodel)
```
```{r}
multipledataset$covariance.ratios = covratio(housingmultimodel)
```


**l.	Perform the necessary calculations to assess the assumption of independence and state if the condition is met or not.**
```{r}
durbinWatsonTest(housingmultimodel)
```
No, it did not meet the condition. Values less than 1 or greater than 3 should definitely raise alarm bells. THe closer to 2 that the value is, the better, but my value is 0.69.

**m.	Perform the necessary calculations to assess the assumption of no multicollinearity and state if the condition is met or not.**
```{r}
vif(housingmultimodel)
```
The largest VIF is not greater than 10, so there is no cause for concern.

```{r}
1/vif(housingmultimodel)
```
Neither of the tolerances are below 0.2 or 0.1, so there is no cause for concern.

```{r}
mean(vif(housingmultimodel))
```
Lastly, the average VIF is greater than 1, but isn't substantially greater than 1.

Based on these measures we can safely conclude that there is no collinearity within our data.

**n.	Visually check the assumptions related to the residuals using the plot() and hist() functions. Summarize what each graph is informing you of and if any anomalies are present.**
```{r}
plot(multipledataset$standardized.residuals)
```


```{r}
hist(rstudent(housingmultimodel))
```
This is a non-normal distribution of studentized residuals.

**o.	Overall, is this regression model unbiased? If an unbiased regression model, what does this tell us about the sample vs. the entire population model?**

Based on the above analysis, this model is indeed bias and doesn't do an effective job of accuratly reresent the sample or the population as a whole.
